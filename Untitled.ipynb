{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35dd85f0-17a9-4fc1-8e0f-227bf5c54558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3fd4b87-b0ad-4b06-81bf-a3608a9cef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_must = \"PMP Government\"\n",
    "is_plus = \"AI DS Management\"\n",
    "w = [2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e7d0dc6-9ff3-498b-8df4-5db726ddbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_must = is_must.split()\n",
    "is_plus = is_plus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5325a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = f\"\"\"\n",
    "site:jo.linkedin.com/in AI AND Machine Learning AND (Transformers OR Self Superfised learning)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c595f7-8b24-40d8-8e1b-03586f73cd66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googlesearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgooglesearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m search\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the query you want to search\u001b[39;00m\n\u001b[0;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124msite:jo.linkedin.com/in \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_must[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AND \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_must[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googlesearch'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "# Define the query you want to search\n",
    "\n",
    "query = f\"\"\"\n",
    "site:jo.linkedin.com/in \"{is_must[0]}\" AND \"{is_must[1]}\"\n",
    "\"\"\"\n",
    "\n",
    "all_res = []\n",
    "for skill_no in range(len(is_plus)):\n",
    "    # prep query\n",
    "    Q = f\"\"\"{query} AND {is_plus[skill_no]}\"\"\".replace('\\n', '')\n",
    "    # Perform the Google search\n",
    "    search_results = search(query)\n",
    "    # Print the URLs of the search results\n",
    "    res = []\n",
    "    for result in search_results:\n",
    "        res.append(result)\n",
    "    \n",
    "# Perform the Google search\n",
    "# search_results = search(query)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "# all_res = []\n",
    "# for result in search_results:\n",
    "    # all_res.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92828733-8fcc-4f2c-afb8-25cbfa73f533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc811d3e-9199-462b-b4db-b68e126a0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "site:jo.linkedin.com/in \"{is_must[0]}\" AND \"{is_must[1]}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7183202-3d66-4dfb-ba35-016989a22d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'site:jo.linkedin.com/in \"PMP\" AND \"Government\" AND AI'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"\"\"{query} AND {is_plus[0]}\"\"\".replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee60ed35-4d00-4ba6-b0f5-15e2cf1d1a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results - Page 1:\n",
      "site:jo.linkedin.com/in \"PMP\" AND \"Government\" AND \"AI\"  \n",
      " https://www.google.com/search?q=site:jo.linkedin.com/in \"PMP\" AND \"Government\" AND \"AI\" &start=0\n",
      "Search results - Page 2:\n",
      "site:jo.linkedin.com/in \"PMP\" AND \"Government\" AND \"AI\"  \n",
      " https://www.google.com/search?q=site:jo.linkedin.com/in \"PMP\" AND \"Government\" AND \"AI\" &start=10\n",
      "Search results - Page 3:\n",
      "site:jo.linkedin.com/in \"PMP\" AND \"Government\" AND \"AI\"  \n",
      " https://www.google.com/search?q=site:jo.linkedin.com/in \"PMP\" AND \"Government\" AND \"AI\" &start=20\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=site:jo.linkedin.com/in%20%22PMP%22%20AND%20%22Government%22%20AND%20%22AI%22%20&start=30 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002C212C98490>, 'Connection to www.google.com timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x000002C212C98490>, 'Connection to www.google.com timed out. (connect timeout=None)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=site:jo.linkedin.com/in%20%22PMP%22%20AND%20%22Government%22%20AND%20%22AI%22%20&start=30 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002C212C98490>, 'Connection to www.google.com timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com/search?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQ\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&start=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m }\n\u001b[1;32m---> 27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Parse the HTML content\u001b[39;00m\n\u001b[0;32m     30\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:553\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=site:jo.linkedin.com/in%20%22PMP%22%20AND%20%22Government%22%20AND%20%22AI%22%20&start=30 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002C212C98490>, 'Connection to www.google.com timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the search query\n",
    "query = f\"\"\"\n",
    "site:jo.linkedin.com/in \"{is_must[0]}\" AND \"{is_must[1]}\"\n",
    "\"\"\"\n",
    "# Set the number of pages to scrape\n",
    "num_pages = 30\n",
    "\n",
    "all_res = []\n",
    "for skill_no in range(len(is_plus)):\n",
    "    # prep query\n",
    "    Q = f\"\"\"{query} AND \"{is_plus[skill_no]}\" \"\"\".replace('\\n', '')\n",
    "    ############################\n",
    "\n",
    "    # Iterate over each page of search results\n",
    "    for page in range(num_pages):\n",
    "        # Calculate the start parameter for pagination\n",
    "        start = page * 10\n",
    "\n",
    "        # Send a request to Google and get the response\n",
    "        url = f\"https://www.google.com/search?q={Q}&start={start}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract search results\n",
    "        search_results = []\n",
    "        for result in soup.find_all(\"div\", class_=\"g\"):\n",
    "            title = result.find(\"h3\", class_=\"LC20lb DKV0Md\").text if result.find(\"h3\", class_=\"LC20lb DKV0Md\") else None\n",
    "            link = result.find(\"a\")[\"href\"] if result.find(\"a\") else None\n",
    "            description = result.find(\"span\", class_=\"aCOpRe\").text if result.find(\"span\", class_=\"aCOpRe\") else None\n",
    "            search_results.append({\"title\": title, \"link\": link, \"description\": description})\n",
    "\n",
    "        # Print search results for the current page\n",
    "        print(f\"Search results - Page {page + 1}:\")\n",
    "        print(Q, \"\\n\", url)\n",
    "        for result in search_results:\n",
    "            print(\"Link:\", result[\"link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c845bae-5f07-476b-99d3-89f3faf9cf4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m search_results \u001b[38;5;241m=\u001b[39m search(query)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the URLs of the search results\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# all_res = []\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:557\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    555\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    556\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 557\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:749\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    746\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    747\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "# Define the query you want to search\n",
    "\n",
    "query = f\"\"\"\n",
    "site:jo.linkedin.com/in \"{is_must[0]}\" AND \"{is_must[1]}\"\n",
    "\"\"\"\n",
    "\n",
    "# Perform the Google search\n",
    "search_results = search(query)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "# all_res = []\n",
    "for result in search_results:\n",
    "    print(result)\n",
    "#     all_res.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402c3d21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=web%20scraping (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C25F632290>, 'Connection to www.google.com timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x000001C25F632290>, 'Connection to www.google.com timed out. (connect timeout=None)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=web%20scraping (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C25F632290>, 'Connection to www.google.com timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Send a request to Google and retrieve the HTML response\u001b[39;00m\n\u001b[0;32m      8\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com/search?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Parse the HTML response using BeautifulSoup\u001b[39;00m\n\u001b[0;32m     12\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:553\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /search?q=web%20scraping (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C25F632290>, 'Connection to www.google.com timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the query to search for\n",
    "query = \"web scraping\"\n",
    "\n",
    "# Send a request to Google and retrieve the HTML response\n",
    "url = f\"https://www.google.com/search?q={query}\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the search results from the parsed HTML\n",
    "results = soup.find_all('div', {'class': 'g'})\n",
    "\n",
    "# Extract the title and URL of each search result\n",
    "for result in results:\n",
    "    title = result.find('h3').text\n",
    "    url = result.find('a')['href']\n",
    "    print(f\"Title: {title}\\nURL: {url}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebbc196",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTTPAdapter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m session \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m     11\u001b[0m retries \u001b[38;5;241m=\u001b[39m Retry(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, backoff_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, status_forcelist\u001b[38;5;241m=\u001b[39m[ \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m502\u001b[39m, \u001b[38;5;241m503\u001b[39m, \u001b[38;5;241m504\u001b[39m ])\n\u001b[1;32m---> 12\u001b[0m session\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mHTTPAdapter\u001b[49m(max_retries\u001b[38;5;241m=\u001b[39mretries))\n\u001b[0;32m     13\u001b[0m session\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m'\u001b[39m, HTTPAdapter(max_retries\u001b[38;5;241m=\u001b[39mretries))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Send a request to Google and retrieve the HTML response with timeout\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTTPAdapter' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import Retry\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests import Session\n",
    "\n",
    "# Define the query to search for\n",
    "query = \"web scraping\"\n",
    "\n",
    "# Create a session and set retry and timeout parameters\n",
    "session = Session()\n",
    "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# Send a request to Google and retrieve the HTML response with timeout\n",
    "url = f\"https://www.google.com/search?q={query}\"\n",
    "response = session.get(url, timeout=5)\n",
    "\n",
    "# Parse the HTML response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the search results from the parsed HTML\n",
    "results = soup.find_all('div', {'class': 'g'})\n",
    "\n",
    "# Extract the title and URL of each search result\n",
    "for result in results:\n",
    "    title = result.find('h3').text\n",
    "    url = result.find('a')['href']\n",
    "    print(f\"Title: {title}\\nURL: {url}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe59fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import Retry, HTTPAdapter\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Define the query to search for\n",
    "query = \"web scraping\"\n",
    "\n",
    "# Create a session and set retry and timeout parameters\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# Send a request to Google and retrieve the HTML response with timeout\n",
    "url = f\"https://www.google.com/search?q={query}\"\n",
    "response = session.get(url, timeout=5)\n",
    "\n",
    "# Parse the HTML response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the search results from the parsed HTML\n",
    "results = soup.find_all('div', {'class': 'g'})\n",
    "\n",
    "# Extract the title and URL of each search result\n",
    "for result in results:\n",
    "    title = result.find('h3').text\n",
    "    url = result.find('a')['href']\n",
    "    print(f\"Title: {title}\\nURL: {url}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf8589d",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb scraping\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Perform the Google search and retrieve the URLs\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search(query):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(url)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:557\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    555\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    556\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 557\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:749\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    746\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    747\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "# Define the query to search for\n",
    "query = \"web scraping\"\n",
    "\n",
    "# Perform the Google search and retrieve the URLs\n",
    "for url in search(query):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "381036e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Define the query to search for\n",
    "query = \"web scraping\"\n",
    "\n",
    "# Send a request to Google and retrieve the HTML response\n",
    "url = f\"https://www.google.com/search?q={query}\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the search results from the parsed HTML\n",
    "results = soup.find_all('div', {'class': 'g'})\n",
    "\n",
    "# Extract the title and URL of each search result\n",
    "for result in results:\n",
    "    title = result.find('h3').text\n",
    "    url = result.find('a')['href']\n",
    "    print(f\"Title: {title}\\nURL: {url}\\n\")\n",
    "    \n",
    "    # Add a delay of 1 second between each request\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e091fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m soup \u001b[38;5;241m=\u001b[39m bs4\u001b[38;5;241m.\u001b[39mBeautifulSoup( request_result\u001b[38;5;241m.\u001b[39mtext \n\u001b[0;32m     16\u001b[0m                          , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Finding temperature in Celsius.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# The temperature is stored inside the class \"BNeawe\". \u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBNeawe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m( temp )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "import requests \n",
    "import bs4 \n",
    "  \n",
    "# Taking thecity name as an input from the user\n",
    "city = \"Imphal\"\n",
    "  \n",
    "# Generating the url  \n",
    "url = \"https://google.com/search?q=weather+in+\" + city\n",
    "  \n",
    "# Sending HTTP request \n",
    "request_result = requests.get( url )\n",
    "  \n",
    "# Pulling HTTP data from internet \n",
    "soup = bs4.BeautifulSoup( request_result.text \n",
    "                         , \"html.parser\" )\n",
    "  \n",
    "# Finding temperature in Celsius.\n",
    "# The temperature is stored inside the class \"BNeawe\". \n",
    "temp = soup.find( \"div\" , class_='BNeawe' ).text \n",
    "    \n",
    "print( temp ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1763ea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [429]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e866a",
   "metadata": {},
   "source": [
    "# Done/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfddb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Using cached google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\abdalrahmanshahrour\\desktop\\googleres\\env\\lib\\site-packages (from google-search-results) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdalrahmanshahrour\\desktop\\googleres\\env\\lib\\site-packages (from requests->google-search-results) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdalrahmanshahrour\\desktop\\googleres\\env\\lib\\site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abdalrahmanshahrour\\desktop\\googleres\\env\\lib\\site-packages (from requests->google-search-results) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdalrahmanshahrour\\desktop\\googleres\\env\\lib\\site-packages (from requests->google-search-results) (2022.12.7)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (pyproject.toml): started\n",
      "  Building wheel for google-search-results (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32076 sha256=0346b0075f0688025c953a541e08eb76465388578fe9450d14db2110d9a10e90\n",
      "  Stored in directory: c:\\users\\abdalrahmanshahrour\\appdata\\local\\pip\\cache\\wheels\\d3\\b2\\c3\\03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68019e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "query = f\"\"\"\n",
    "site:jo.linkedin.com/in AI AND Machine Learning OR Transformers OR Self Superfised learning\n",
    "\"\"\"\n",
    "\n",
    "search = GoogleSearch({\n",
    "    \"q\": query, \n",
    "    \"api_key\": \"1835fadd674bcb5f97ec8bbf2c66cd388599e307ab40ab27f3d49b0f8345e107\",\n",
    "    \"num\": \"30\"\n",
    "  })\n",
    "result = search.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f58f753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': None,\n",
       " 'e': None,\n",
       " 'a': None,\n",
       " 'r': None,\n",
       " 'c': None,\n",
       " 'h': None,\n",
       " '_': None,\n",
       " 'm': None,\n",
       " 't': None,\n",
       " 'd': None}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.fromkeys('search_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6971241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6b4d99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'link' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlink\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'link' is not defined"
     ]
    }
   ],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df9ffaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_metadata\n",
      "search_parameters\n",
      "search_information\n",
      "inline_images\n",
      "organic_results\n",
      "pagination\n",
      "serpapi_pagination\n"
     ]
    }
   ],
   "source": [
    "# position\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b598d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': None,\n",
       " 'e': None,\n",
       " 'a': None,\n",
       " 'r': None,\n",
       " 'c': None,\n",
       " 'h': None,\n",
       " '_': None,\n",
       " 'm': None,\n",
       " 't': None,\n",
       " 'd': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get(\"search_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "138aa61f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_metadata\n",
      "search_parameters\n",
      "search_information\n",
      "inline_images\n",
      "organic_results\n",
      "pagination\n",
      "serpapi_pagination\n"
     ]
    }
   ],
   "source": [
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d9ae889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 1,\n",
       "  'title': 'Razan Abdul_Nabi - Data Scientist - Qistas - ',\n",
       "  'link': 'https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '1.5+ years of experience in IT and comprehensive industry knowledge of deep learning, machine learning, Artificial intelligence, Statistical modeling, ...',\n",
       "  'snippet_highlighted_words': ['deep learning',\n",
       "   'machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339e23a5ddce3f73da6017ffdc88aea72a6995b2b332d1b954507edaaa635063d37.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7&tbm=ilp&ilps=ADJL0iyA8_f6WCqDt_t5dQUsqAp5oOx_7Q',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyA8_f6WCqDt_t5dQUsqAp5oOx_7Q&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Frazan-abdul-nabi-51595a1b7',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 2,\n",
       "  'title': 'Osama Fityani - Computer Vision Research Assistant',\n",
       "  'link': 'https://jo.linkedin.com/in/osama-fityani',\n",
       "  'displayed_link': 'https://jo.linkedin.com  osama-fity...',\n",
       "  'snippet': 'Mechatronics Engineer, experienced in Machine Learning, Robotics, and Automation. ... Using its OCR (Optical Character Recognition) deep learning AI model, ...',\n",
       "  'snippet_highlighted_words': ['Machine Learning', 'deep learning AI'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Computer Vision Research Assistant',\n",
       "     'University of Jordan']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/osama-fityani',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133910070d2aa16751c349db844d82688c623c515b567de66cbe7e365cb22cec59b9.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/osama-fityani&tbm=ilp&ilps=ADJL0ixF6B0epLuO2Yjbnog6-zZHZmC0_A',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0ixF6B0epLuO2Yjbnog6-zZHZmC0_A&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fosama-fityani',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 3,\n",
       "  'title': 'Mohammad Zahrawi - Academic Lead & Data Analyst',\n",
       "  'link': 'https://jo.linkedin.com/in/mohammad-zahrawi-11a0a2133/de',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '-Development of ML/AI solutions on leveraging python, fastai and pytorch for spine ... Techniques: MONAI, self-supervised learning,gradcam, augmentation, ...',\n",
       "  'snippet_highlighted_words': ['ML', 'AI', 'self', 'supervised learning'],\n",
       "  'rich_snippet': {'top': {'extensions': ['Academic Lead & Data Analyst',\n",
       "     'Ministry of Education  UAE']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/mohammad-zahrawi-11a0a2133/de',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d913390b9aa8981623fbfee4511d51da49df741d099e6f8f11671bb0b1bc715ec1151e.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/mohammad-zahrawi-11a0a2133/de&tbm=ilp&ilps=ADJL0izJa2-biGLsGkFZcryS04sXuSTf8A',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0izJa2-biGLsGkFZcryS04sXuSTf8A&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fmohammad-zahrawi-11a0a2133%2Fde',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 4,\n",
       "  'title': 'Abdel Rahman AlSabbagh - Research Assistant',\n",
       "  'link': 'https://jo.linkedin.com/in/abdelrahmanalsabbagh',\n",
       "  'displayed_link': 'https://jo.linkedin.com  abdelrahm...',\n",
       "  'snippet': 'My main technical focus revolves around Computational Intelligence, Machine Learning in Generative Modeling, involving Sequence Modeling for both imagery and ...',\n",
       "  'snippet_highlighted_words': ['Machine Learning'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Research Assistant',\n",
       "     'University of Jordan']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/abdelrahmanalsabbagh',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133918198df992c607a7cfb3c1cb592925dc7ddabb9db502ab5c86640239dc1f7bff.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/abdelrahmanalsabbagh&tbm=ilp&ilps=ADJL0iwu6S0s21k1K6F2tOUWGgKkoI6Wfg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwu6S0s21k1K6F2tOUWGgKkoI6Wfg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fabdelrahmanalsabbagh',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 5,\n",
       "  'title': 'Rand Muhtaseb - Goethe-Institut Jordanien -  ',\n",
       "  'link': 'https://jo.linkedin.com/in/randmuhtaseb',\n",
       "  'displayed_link': 'https://jo.linkedin.com  randmuhta...',\n",
       "  'snippet': 'MBZUAI (Mohamed bin Zayed University of Artificial Intelligence) ... In this paper, We propose a self supervised contrastive learning method to segment the ...',\n",
       "  'snippet_highlighted_words': ['Artificial Intelligence',\n",
       "   'self supervised',\n",
       "   'learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/randmuhtaseb',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133935dcc1f0645e33c39b9b4d2a77c3a50a8f827d30b6fabaf8b1e0aa71630be263.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/randmuhtaseb&tbm=ilp&ilps=ADJL0izbJ-MDnyBl7y7DuLKkKiWWIJ-Ymg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0izbJ-MDnyBl7y7DuLKkKiWWIJ-Ymg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Frandmuhtaseb',\n",
       "  'cached_page_link': '/search?ucbcb=1&q=related:https://jo.linkedin.com/in/randmuhtaseb',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 6,\n",
       "  'title': 'Hasan Mohammad Al-Oqool - Artificial Intelligence Trainer',\n",
       "  'link': 'https://jo.linkedin.com/in/hasan-mohammad-al-oqool-8717b9191',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Machine Learning Engineer (Artificial Intelligence Trainer at Tahaluf) ... knowledge I have about these fields are all self-studied and University courses.',\n",
       "  'snippet_highlighted_words': ['Machine Learning',\n",
       "   'Artificial Intelligence',\n",
       "   'self'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Artificial Intelligence Trainer',\n",
       "     'Tahaluf Al Emarat Technical Solutions    ']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/hasan-mohammad-al-oqool-8717b9191',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339ace5ed1b9287f33665cd5a8f6aa6409e53a83669e51992b0ecd995a10e6ead3f.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/hasan-mohammad-al-oqool-8717b9191&tbm=ilp&ilps=ADJL0iySjr9kCvbGaAN0NcIt5jxAu29XOg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iySjr9kCvbGaAN0NcIt5jxAu29XOg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fhasan-mohammad-al-oqool-8717b9191',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 7,\n",
       "  'title': 'Yara Al-Rahahleh - Developer Associate - PwC Middle East',\n",
       "  'link': 'https://jo.linkedin.com/in/yara-al-rahahleh',\n",
       "  'displayed_link': 'https://jo.linkedin.com  yara-al-rahahleh',\n",
       "  'snippet': 'I am a motivated fresh graduate computer engineer, interested in artificial intelligence and machine learning products. Tweaked deep-learning systems and ...',\n",
       "  'snippet_highlighted_words': ['artificial intelligence and machine learning',\n",
       "   'deep',\n",
       "   'learning'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Developer Associate',\n",
       "     'PwC Middle East']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/yara-al-rahahleh',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339def81a56d79228943f3128f730e80804e91394982203d4acbb6752be3827f718.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/yara-al-rahahleh&tbm=ilp&ilps=ADJL0iwgea88oZh1fBGb4fK4TcWsUyot3A',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwgea88oZh1fBGb4fK4TcWsUyot3A&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fyara-al-rahahleh',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 8,\n",
       "  'title': 'Abdalrahman Shahrour - Artificial Intelligence Researcher',\n",
       "  'link': 'https://jo.linkedin.com/in/shahrour',\n",
       "  'displayed_link': 'https://jo.linkedin.com  shahrour',\n",
       "  'snippet': '  Deep Learning for Natural Language Processing    Transformers in Computer Vision    IBM AI Engineering Professional Certificate (V2) ...',\n",
       "  'snippet_highlighted_words': ['Deep Learning', 'Transformers', 'AI'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/shahrour',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133919818818f020af4a2df399dbfba8c88db3be68f606d90c886c9bd9facf70b2bb.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/shahrour&tbm=ilp&ilps=ADJL0iyTFZIp3ILYrBhUFzDpoJ9WlSyO-w',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyTFZIp3ILYrBhUFzDpoJ9WlSyO-w&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fshahrour',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 9,\n",
       "  'title': 'Ayman Al-Bitar - Data Scientist - Orange Jordan',\n",
       "  'link': 'https://jo.linkedin.com/in/ayman-al-bitar-7208baa',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Data Scientist | MLOPS| Deep Learning | ERP techno-functional Consultant ...   Data Science: Supervised Machine Learning in Python ...',\n",
       "  'snippet_highlighted_words': ['Deep Learning',\n",
       "   'Supervised Machine Learning'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Data Scientist',\n",
       "     'Orange Jordan']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/ayman-al-bitar-7208baa',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339eb54c053f35e524938f42a78741bcd660a3699b8baae85bf0e3985f547d78905.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/ayman-al-bitar-7208baa&tbm=ilp&ilps=ADJL0iw7t0JUvrOYla8K-rFlNHIXN20HTA',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iw7t0JUvrOYla8K-rFlNHIXN20HTA&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fayman-al-bitar-7208baa',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 10,\n",
       "  'title': 'Ibrahim Shahbaz - Data Scientist - Open Insights',\n",
       "  'link': 'https://jo.linkedin.com/in/ibrahim-shahbaz-5a428aaa',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '*Responsible for delivering content of Python,R, Machine Learning, Data Engineering ... approaches (e.g., Python) and self-serviced tools (e.g., Alteryx).',\n",
       "  'snippet_highlighted_words': ['Machine Learning', 'self'],\n",
       "  'rich_snippet': {'top': {'extensions': ['  ',\n",
       "     'Data Scientist',\n",
       "     'Open Insights']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/ibrahim-shahbaz-5a428aaa',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339d0d4583c261188c2f6ec9c44f1af964469a05cf6b82877a1216d4cbbce10089e.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/ibrahim-shahbaz-5a428aaa&tbm=ilp&ilps=ADJL0iy9qpK2ZM5PiKE2WoF_pYrId4jjmw',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iy9qpK2ZM5PiKE2WoF_pYrId4jjmw&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fibrahim-shahbaz-5a428aaa',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 11,\n",
       "  'title': 'Esra Shrouf - Pentester (Red Teamer) - Synack Red Team',\n",
       "  'link': 'https://jo.linkedin.com/in/esra-shrouf-35a319137',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Other Skills: --- refer to CV Artificial Intelligence Robotics & Electronics Data ... The major role was self learning, and hence learned Machine Learning, ...',\n",
       "  'snippet_highlighted_words': ['Artificial Intelligence',\n",
       "   'self learning',\n",
       "   'Machine Learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/esra-shrouf-35a319137',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d913395c37632990960fd8804c3b72b4500d012961adcfac2e5630a9caf6bae94207ac2f4ed4e09af955af.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/esra-shrouf-35a319137&tbm=ilp&ilps=ADJL0izlru7LeEFN60AmMUrB1Qsn2QiQCA',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0izlru7LeEFN60AmMUrB1Qsn2QiQCA&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fesra-shrouf-35a319137',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 12,\n",
       "  'title': 'Latif Ullah Khan - Beamline Scientist - SESAME',\n",
       "  'link': 'https://jo.linkedin.com/in/latif-ullah-khan-711b95124',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'In the past decade, machine learning has given us self-driving cars, practical speech ...   Python for Data Science, AI &amp; Development ...',\n",
       "  'snippet_highlighted_words': ['machine learning', 'self', 'AI'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/latif-ullah-khan-711b95124',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339191c7fa14dc01161b0db5e81eb16d11b77214b84476f60c1112d95993f47e61a2b01023fcc5eaba3.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/latif-ullah-khan-711b95124&tbm=ilp&ilps=ADJL0iwaOGzfB0OvbTpb-o0USkpf1FZdag',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwaOGzfB0OvbTpb-o0USkpf1FZdag&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Flatif-ullah-khan-711b95124',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 13,\n",
       "  'title': 'Anas Blasi - Associate Professor of Computer Science',\n",
       "  'link': 'https://jo.linkedin.com/in/anasblasi',\n",
       "  'displayed_link': 'https://jo.linkedin.com  anasblasi',\n",
       "  'snippet': 'This research agenda is centered around the use of Machine Learning (ML) and Artificial Intelligence (AI) in different areas such as applying ML algorithms on ...',\n",
       "  'snippet_highlighted_words': ['Machine Learning',\n",
       "   'ML',\n",
       "   'Artificial Intelligence',\n",
       "   'AI',\n",
       "   'ML'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/anasblasi',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339212d1bcc35de34cfd00216fd821cca3120100ad7f1e6ed7c716d07ed9b4d9e4f16d217920c99837c.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/anasblasi&tbm=ilp&ilps=ADJL0izb0nWRDayg3q94nX3l_XgmdLMb_w',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0izb0nWRDayg3q94nX3l_XgmdLMb_w&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fanasblasi',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 14,\n",
       "  'title': 'Rami Al Karmi - Founder & Executive Chairman',\n",
       "  'link': 'https://jo.linkedin.com/in/ramialkarmi',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ramialkarmi',\n",
       "  'snippet': 'I am also the Director of The Online Learning Center @ HTU, ... renewable energy, artificial intelligence & machine learning), with the future of work and ...',\n",
       "  'snippet_highlighted_words': ['Learning',\n",
       "   'artificial intelligence',\n",
       "   'machine learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/ramialkarmi',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339a4c91fc699b4781acd292a2202b62ba2d92adbfecbc761564e9d452948d22a819387bf780465d7e2.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/ramialkarmi&tbm=ilp&ilps=ADJL0iyQzPKEfaN2Pa1jjdmkqxnrMH67Zg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyQzPKEfaN2Pa1jjdmkqxnrMH67Zg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Framialkarmi',\n",
       "  'cached_page_link': '/search?ucbcb=1&q=related:https://jo.linkedin.com/in/ramialkarmi',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 15,\n",
       "  'title': 'Ali Safia - Data Science and AI Team Leader - Zain',\n",
       "  'link': 'https://jo.linkedin.com/in/alisafia',\n",
       "  'displayed_link': 'https://jo.linkedin.com  alisafia',\n",
       "  'snippet': 'End-to-end Jordanian dialect speech-to-text self-supervised learning framework. Frontiers in AI and robotics \\u200f20  2022  An Online Machine Learning ...',\n",
       "  'snippet_highlighted_words': ['self',\n",
       "   'supervised learning',\n",
       "   'AI',\n",
       "   'Machine Learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/alisafia',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339b9291d3481d81d9e737f4ba2903b87c35cf7abb9173fcd4a32f1df710949dea73fa45b878cd644fc.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/alisafia&tbm=ilp&ilps=ADJL0iyqb0FR5jTba7ZyayMucZYfFk9xVw',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyqb0FR5jTba7ZyayMucZYfFk9xVw&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Falisafia',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 16,\n",
       "  'title': 'Raneem Qaddoura - Al Hussein Technical University (HTU)',\n",
       "  'link': 'https://jo.linkedin.com/in/raneem-qaddoura-97623a1a',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Papers published in JCR, ISI, Scopus Journals, Book Chapters, and Conferences. Research interests involve machine learning, data science, and evolutionary ...',\n",
       "  'snippet_highlighted_words': ['machine learning'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Assistant Professor of AI and Data Science',\n",
       "     'Al Hussein Technical University (HTU)']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/raneem-qaddoura-97623a1a',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133923f12f64565183a07ff1ae2315ffe3669a46fd794db215d5ee8f1ae115be031a57ced33c5a5b9ea6.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/raneem-qaddoura-97623a1a&tbm=ilp&ilps=ADJL0iyOxGhGVZaLOcUD4tmEIt8wq7x1RQ',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyOxGhGVZaLOcUD4tmEIt8wq7x1RQ&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Franeem-qaddoura-97623a1a',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 17,\n",
       "  'title': 'Odii Fakher - Artificial Intelligence Researcher',\n",
       "  'link': 'https://jo.linkedin.com/in/odii-fakher-778045148',\n",
       "  'displayed_link': 'https://jo.linkedin.com  odii-fakhe...',\n",
       "  'snippet': 'Artificial Intelligence Researcher ... Machine Learning Researcher ... implement analytical solutions & techniques independently or with minimal supervision',\n",
       "  'snippet_highlighted_words': ['Artificial Intelligence',\n",
       "   'Machine Learning',\n",
       "   'supervision'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/odii-fakher-778045148',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133915c231994496677309f5425b728424e5b45ce0d537985c25742620e2e07471c0091adec58049448c.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/odii-fakher-778045148&tbm=ilp&ilps=ADJL0ixaGKK9fEy8atuSob3CdZf_O_Kx1g',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0ixaGKK9fEy8atuSob3CdZf_O_Kx1g&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fodii-fakher-778045148',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 18,\n",
       "  'title': 'Mohammad Sheri, AI, CMRP',\n",
       "  'link': 'https://jo.linkedin.com/in/mohammad-sheri-ai-cmrp-61020b1a2',\n",
       "  'displayed_link': 'https://jo.linkedin.com  mohamma...',\n",
       "  'snippet': '     Mohammad Sheri, AI, CMRP   ... A Cookbook of Self-Supervised Learning ...   Applied Machine Learning ...',\n",
       "  'snippet_highlighted_words': ['AI',\n",
       "   'Self',\n",
       "   'Supervised Learning',\n",
       "   'Machine Learning'],\n",
       "  'rich_snippet': {'top': {'extensions': ['  ',\n",
       "     'Electrical Maintenance Engineer',\n",
       "     '    - Samra Electric Power Co.']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/mohammad-sheri-ai-cmrp-61020b1a2',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339f2560e158e7b3b340e12e8d1c1075d638529e2788ca57c783f99eea21eef49000a2d174f413321a6.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/mohammad-sheri-ai-cmrp-61020b1a2&tbm=ilp&ilps=ADJL0iwJFYu0XzfAdnflfmx2fYpSZJsVaQ',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwJFYu0XzfAdnflfmx2fYpSZJsVaQ&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fmohammad-sheri-ai-cmrp-61020b1a2',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 19,\n",
       "  'title': 'Msc Arar Al Tawil - BTEC lecturer',\n",
       "  'link': 'https://jo.linkedin.com/in/msc-arar-al-tawil-990b5b14a',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Introduction to TensorFlow for Artificial Intelligence, Machine Learning ... Supervised learning algorithms can \"learn\" from labeled dataset and then detect ...',\n",
       "  'snippet_highlighted_words': ['Artificial Intelligence',\n",
       "   'Machine Learning',\n",
       "   'Supervised learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/msc-arar-al-tawil-990b5b14a',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133949cafb0851531a911e3b63e92836a17fadcf47a5dc48fda21df15f8a61c25543ee661e713334305e.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/msc-arar-al-tawil-990b5b14a&tbm=ilp&ilps=ADJL0iztCbgcUVgMlnr7n5grw4mXZXcR5A',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iztCbgcUVgMlnr7n5grw4mXZXcR5A&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fmsc-arar-al-tawil-990b5b14a',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 20,\n",
       "  'title': 'Israa Al-Dayyat - Research Analyst trainee - Amazon',\n",
       "  'link': 'https://jo.linkedin.com/in/israa-al-dayyat-103769188',\n",
       "  'displayed_link': 'https://jo.linkedin.com  israa-al-da...',\n",
       "  'snippet': 'RA AcademyArtificial Intelligence. 2022 - 2022.  :Statistics || Python || Machine Learning (Supervised & Unsupervised learning, ...',\n",
       "  'snippet_highlighted_words': ['Artificial Intelligence',\n",
       "   'Machine Learning',\n",
       "   'Supervised',\n",
       "   'learning'],\n",
       "  'rich_snippet': {'top': {'extensions': ['',\n",
       "     'Research Analyst trainee',\n",
       "     'Amazon']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/israa-al-dayyat-103769188',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339bb29ae00fb074a90795e2f51a117ebfd093dfb1a1368c21535c69a48119eaa052de9efca247b6250.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/israa-al-dayyat-103769188&tbm=ilp&ilps=ADJL0ize50Vn1NrhA_BdtYDkOrAyCUrtYw',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0ize50Vn1NrhA_BdtYDkOrAyCUrtYw&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fisraa-al-dayyat-103769188',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 21,\n",
       "  'title': 'Omar Aloyoun - Startups Adviser - Newchip Accelerator',\n",
       "  'link': 'https://jo.linkedin.com/in/omaraloyoun',\n",
       "  'displayed_link': 'https://jo.linkedin.com  omaraloyoun',\n",
       "  'snippet': 'Having 22 years of diverse experience across different industries: online media, web/mobile technologies, IoT, AI/machine learning/bots, Telecoms & ICT, ...',\n",
       "  'snippet_highlighted_words': ['AI', 'machine learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/omaraloyoun',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133955c81e2b6d596a766873108d7afe2a6fc288b632f60afba2705dd30b12bb4f8514f11b79db1e69c5.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/omaraloyoun&tbm=ilp&ilps=ADJL0iwr1MqYcznU13bS_AmHqs-L8b0Z-g',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwr1MqYcznU13bS_AmHqs-L8b0Z-g&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fomaraloyoun',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 22,\n",
       "  'title': 'Bashar Talafha - Graduate Research Assistant',\n",
       "  'link': 'https://jo.linkedin.com/in/bashar-talafha',\n",
       "  'displayed_link': 'https://jo.linkedin.com  bashar-talafha',\n",
       "  'snippet': 'A developer and researcher at Samsung R&D Institute Jordan in the department of Artificial Intelligence who interested in applying Machine learning and Deep ...',\n",
       "  'snippet_highlighted_words': ['Artificial Intelligence', 'Machine learning'],\n",
       "  'rich_snippet': {'top': {'extensions': ['',\n",
       "     'Graduate Research Assistant',\n",
       "     'The University of British Columbia']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/bashar-talafha',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339e3c2a79fd308fe0b1750d2e743c15ce514f255d3a52b2e0acf01a4661baf1b0b0017bb3b439248a8.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/bashar-talafha&tbm=ilp&ilps=ADJL0iwHcnRMOT5EEaSgMN87jYy9KOmUvA',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwHcnRMOT5EEaSgMN87jYy9KOmUvA&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fbashar-talafha',\n",
       "  'missing': ['(Transformers'],\n",
       "  'must_include': {'word': '(Transformers',\n",
       "   'link': 'https://www.google.com/search?num=30&ucbcb=1&q=site:jo.linkedin.com/in+AI+AND+Machine+Learning+AND+%22(Transformers%22+OR+Self+Supervised+learning)&sa=X&ved=2ahUKEwji-bWM3tP-AhUahIkEHZCgCCsQ5t4CegQIWxAB'},\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 23,\n",
       "  'title': 'Haneen Alakhrass - Trainer - Tahaluf Al Emarat Technical ...',\n",
       "  'link': 'https://jo.linkedin.com/in/haneen-alakhrass-b78a4b213',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '... along with Machine learning approaches including supervised and ... MSc(Eng) - AI trainer at Tahaluf AlEmarat ...   Freelance (Self employed) ...',\n",
       "  'snippet_highlighted_words': ['Machine learning',\n",
       "   'supervised',\n",
       "   'AI',\n",
       "   'Self'],\n",
       "  'rich_snippet': {'top': {'extensions': ['  ',\n",
       "     'Trainer',\n",
       "     'Tahaluf Al Emarat Technical Solutions    ']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/haneen-alakhrass-b78a4b213',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133989288c7a9a19d29bae9d1958d175e077622ae3bacbd312e136a62a58295129f5d6d2bcfe1bc89b77.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/haneen-alakhrass-b78a4b213&tbm=ilp&ilps=ADJL0iy0MUNO8KTr-Fq6xRHzIZOaoFnryA',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iy0MUNO8KTr-Fq6xRHzIZOaoFnryA&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fhaneen-alakhrass-b78a4b213',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 24,\n",
       "  'title': 'Mohammad Bsharat - Techinical Trainer - Arab Potash ...',\n",
       "  'link': 'https://jo.linkedin.com/in/mohammad-bsharat-347674195',\n",
       "  'displayed_link': 'https://jo.linkedin.com  mohamma...',\n",
       "  'snippet': '  Supervised Learning with scikit-learn ... comparison study on the traditional and artificial intelligence methods of designing special type of ...',\n",
       "  'snippet_highlighted_words': ['Supervised Learning',\n",
       "   'artificial intelligence'],\n",
       "  'rich_snippet': {'top': {'extensions': ['',\n",
       "     'Techinical Trainer',\n",
       "     'Arab Potash Company']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/mohammad-bsharat-347674195',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d9133938f1c7998f34bee80ea7e0b152debbf41e1d56bf6987aaf0b950ee76c5dd0d9c143180753b8b1563.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/mohammad-bsharat-347674195&tbm=ilp&ilps=ADJL0iymlWIvkWhOzBfPhxzrpt4Yphxyyg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iymlWIvkWhOzBfPhxzrpt4Yphxyyg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fmohammad-bsharat-347674195',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 25,\n",
       "  'title': 'Ahmad Obaidat - IT Technician - Jordan University of ...',\n",
       "  'link': 'https://jo.linkedin.com/in/ahmad-obaidat-99184ab0',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '2-Supervision of the self-service devices in the library (3m self check service ... Aspiring Machine Learning Engineer - MSIT @WU | Artificial Intelligence ...',\n",
       "  'snippet_highlighted_words': ['Supervision',\n",
       "   'self',\n",
       "   'self',\n",
       "   'Machine Learning',\n",
       "   'Artificial Intelligence'],\n",
       "  'rich_snippet': {'top': {'extensions': ['',\n",
       "     'IT Technician',\n",
       "     'Jordan University of Science and Technology']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/ahmad-obaidat-99184ab0',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339b703df52268355f8d181d80837137f26a587ccce86f66b86ec807c9778d37a362eb1dfa571ca2f18.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/ahmad-obaidat-99184ab0&tbm=ilp&ilps=ADJL0iy7pi9mWBal2Gd4HPUvs9hq3JSD6Q',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iy7pi9mWBal2Gd4HPUvs9hq3JSD6Q&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fahmad-obaidat-99184ab0',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 26,\n",
       "  'title': 'Qusay AlSmadi - Product Manager - Mawarid Media & ...',\n",
       "  'link': 'https://jo.linkedin.com/in/qusayalsmadi',\n",
       "  'displayed_link': 'https://jo.linkedin.com  qusayalsm...',\n",
       "  'snippet': 'Building Recommender Systems with Machine Learning and AI  LinkedIn.    \\u200f 2020.    .   Machine Learning and AI ...',\n",
       "  'snippet_highlighted_words': ['Machine Learning',\n",
       "   'AI',\n",
       "   'Machine Learning',\n",
       "   'AI'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Product Manager',\n",
       "     'Mawarid Media & Communications Group']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/qusayalsmadi',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d91339156a9a0754f4544250846c541be0d6c9027dd8bd83f3de2045f5c0529dac5026e8b1d2f04d55848f.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/qusayalsmadi&tbm=ilp&ilps=ADJL0iwv1fuzEIR_OBLIu19tMOllaMmNBQ',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwv1fuzEIR_OBLIu19tMOllaMmNBQ&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fqusayalsmadi',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 27,\n",
       "  'title': 'Ibrahim Barqawi - Management Consultant',\n",
       "  'link': 'https://jo.linkedin.com/in/ibrahim-barqawi-24b84116b',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Conducting checking for all transformers, switches, circuit breakers, and insulations ... Supervised Machine Learning: Regression and Classification.',\n",
       "  'snippet_highlighted_words': ['transformers',\n",
       "   'Supervised Machine Learning',\n",
       "   'Classification'],\n",
       "  'rich_snippet': {'top': {'extensions': ['      ',\n",
       "     'Management Consultant',\n",
       "     'Sundus Recruitment and Outsourcing Services - Executive Search and Recruitment Agency']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/ibrahim-barqawi-24b84116b',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d913396e2a2a0529a844cd4b8711d51ce1f391522cb2c6d1968d033e6c514d4f0a3f1fb0a934c68ab2b10d.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/ibrahim-barqawi-24b84116b&tbm=ilp&ilps=ADJL0iz8iu82s1ZTIFO1XNFLZxZ6DumJ3Q',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iz8iu82s1ZTIFO1XNFLZxZ6DumJ3Q&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fibrahim-barqawi-24b84116b',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 28,\n",
       "  'title': 'Zaid Arabiyat - Director of Operations - Aces Acquisitions',\n",
       "  'link': 'https://jo.linkedin.com/in/zaid-arabiyat-27309317b',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Experienced in Machine Learning and AI. ... and probability to real-world scenarios, such as analyzing A/B tests and building supervised learning models.',\n",
       "  'snippet_highlighted_words': ['Machine Learning',\n",
       "   'AI',\n",
       "   'supervised learning'],\n",
       "  'rich_snippet': {'top': {'extensions': ['    ',\n",
       "     'Director of Operations',\n",
       "     'Aces Acquisitions']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/zaid-arabiyat-27309317b',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d913398c78ca4dcb145a502b1a1d5431e9d4a912905916713bad7f9e2d70a47021b179a67eab9d5f161602.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/zaid-arabiyat-27309317b&tbm=ilp&ilps=ADJL0iyk34nqZbiowe2tGxLYTGdp-ndw7Q',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyk34nqZbiowe2tGxLYTGdp-ndw7Q&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fzaid-arabiyat-27309317b',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 29,\n",
       "  'title': 'Tariq Abu-Rashed -  |   ',\n",
       "  'link': 'https://jo.linkedin.com/in/tariq-abu-rashed-a3495b89',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '24 AI tools to future-proof yourself ... Supervision of   ... Difficulties Faced and Applications of Machine Learning in Cyber-Security.',\n",
       "  'snippet_highlighted_words': ['AI', 'Supervision', 'Machine Learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/tariq-abu-rashed-a3495b89',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f7cbc2f542eee93e8de7c/images/ac3ecf5fdec9c9db350d1e11f3d913399d9d075ebec6be3c0710ac8a1b5ad16d483d290b3e079fb2d9a4a023dec8845453533352c4d85769.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/tariq-abu-rashed-a3495b89&tbm=ilp&ilps=ADJL0izwsqKtgSXWSO3NaqFyZIZHu-SatQ',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0izwsqKtgSXWSO3NaqFyZIZHu-SatQ&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Ftariq-abu-rashed-a3495b89',\n",
       "  'source': 'linkedin.com'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get(\"organic_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ed37648",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item  = result.get(\"organic_results\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1d4db9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'position': 1,\n",
       " 'title': 'Razan Abdul_Nabi - Data Scientist - Qistas - ',\n",
       " 'link': 'https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7',\n",
       " 'displayed_link': 'https://jo.linkedin.com  ...',\n",
       " 'snippet': '1.5+ years of experience in IT and comprehensive industry knowledge of deep learning, machine learning, Artificial intelligence, Statistical modeling, ...',\n",
       " 'snippet_highlighted_words': ['deep learning',\n",
       "  'machine learning',\n",
       "  'Artificial intelligence'],\n",
       " 'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "   'source_info_link': 'https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7',\n",
       "   'security': 'secure',\n",
       "   'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac3a5ec26451a013605dab64c548c19b0351f5bc967da3a0da3729fb719098bb11e.png'}},\n",
       " 'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7&tbm=ilp&ilps=ADJL0iyA8_f6WCqDt_t5dQUsqAp5oOx_7Q',\n",
       " 'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyA8_f6WCqDt_t5dQUsqAp5oOx_7Q&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Frazan-abdul-nabi-51595a1b7',\n",
       " 'source': 'linkedin.com'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c99b5a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7\n",
      "Razan Abdul_Nabi  [' Data Scientist ', ' Qistas ', ' ']\n",
      "['deep learning', 'machine learning', 'Artificial intelligence']\n",
      "1.5+ years of experience in IT and comprehensive industry knowledge of deep learning, machine learning, Artificial intelligence, Statistical modeling, ...\n"
     ]
    }
   ],
   "source": [
    "print(first_item.get(\"link\"))\n",
    "x = first_item.get(\"title\").split('-')\n",
    "print(x[0], x[1:])\n",
    "print(first_item.get(\"snippet_highlighted_words\"))\n",
    "print(first_item.get(\"snippet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14888292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 1,\n",
       "  'title': 'Razan Abdul_Nabi - Data Scientist - Qistas - ',\n",
       "  'link': 'https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '1.5+ years of experience in IT and comprehensive industry knowledge of deep learning, machine learning, Artificial intelligence, Statistical modeling, ...',\n",
       "  'snippet_highlighted_words': ['deep learning',\n",
       "   'machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac3a5ec26451a013605dab64c548c19b0351f5bc967da3a0da3729fb719098bb11e.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/razan-abdul-nabi-51595a1b7&tbm=ilp&ilps=ADJL0iyA8_f6WCqDt_t5dQUsqAp5oOx_7Q',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyA8_f6WCqDt_t5dQUsqAp5oOx_7Q&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Frazan-abdul-nabi-51595a1b7',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 2,\n",
       "  'title': 'Osama Fityani - Computer Vision Research Assistant',\n",
       "  'link': 'https://jo.linkedin.com/in/osama-fityani',\n",
       "  'displayed_link': 'https://jo.linkedin.com  osama-fity...',\n",
       "  'snippet': 'Mechatronics Engineer, experienced in Machine Learning, Robotics, and Automation. ... Using its OCR (Optical Character Recognition) deep learning AI model, ...',\n",
       "  'snippet_highlighted_words': ['Machine Learning', 'deep learning AI'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Computer Vision Research Assistant',\n",
       "     'University of Jordan']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/osama-fityani',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac3d626e5831dba7b8e74ff43eabe98b08727e516a50bfdc183d26c41b43a6d5d6a.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/osama-fityani&tbm=ilp&ilps=ADJL0ixF6B0epLuO2Yjbnog6-zZHZmC0_A',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0ixF6B0epLuO2Yjbnog6-zZHZmC0_A&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fosama-fityani',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 3,\n",
       "  'title': 'Mohammad Zahrawi - Academic Lead & Data Analyst',\n",
       "  'link': 'https://jo.linkedin.com/in/mohammad-zahrawi-11a0a2133/de',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': '-Development of ML/AI solutions on leveraging python, fastai and pytorch for spine ... Techniques: MONAI, self-supervised learning,gradcam, augmentation, ...',\n",
       "  'snippet_highlighted_words': ['ML', 'AI', 'self', 'supervised learning'],\n",
       "  'rich_snippet': {'top': {'extensions': ['Academic Lead & Data Analyst',\n",
       "     'Ministry of Education  UAE']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/mohammad-zahrawi-11a0a2133/de',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac37d045f1392c9fa089ffcf435130cf00ba37ea1d5b879174df5e1eaec3e4ee31c.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/mohammad-zahrawi-11a0a2133/de&tbm=ilp&ilps=ADJL0izJa2-biGLsGkFZcryS04sXuSTf8A',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0izJa2-biGLsGkFZcryS04sXuSTf8A&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fmohammad-zahrawi-11a0a2133%2Fde',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 4,\n",
       "  'title': 'Abdel Rahman AlSabbagh - Research Assistant',\n",
       "  'link': 'https://jo.linkedin.com/in/abdelrahmanalsabbagh',\n",
       "  'displayed_link': 'https://jo.linkedin.com  abdelrahm...',\n",
       "  'snippet': 'My main technical focus revolves around Computational Intelligence, Machine Learning in Generative Modeling, involving Sequence Modeling for both imagery and ...',\n",
       "  'snippet_highlighted_words': ['Machine Learning'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Research Assistant',\n",
       "     'University of Jordan']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/abdelrahmanalsabbagh',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac331931a956303830247640f4a3b61d0991bd39450fa85ff2ea06641781c767e33.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/abdelrahmanalsabbagh&tbm=ilp&ilps=ADJL0iwu6S0s21k1K6F2tOUWGgKkoI6Wfg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwu6S0s21k1K6F2tOUWGgKkoI6Wfg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fabdelrahmanalsabbagh',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 5,\n",
       "  'title': 'Rand Muhtaseb - Goethe-Institut Jordanien -  ',\n",
       "  'link': 'https://jo.linkedin.com/in/randmuhtaseb',\n",
       "  'displayed_link': 'https://jo.linkedin.com  randmuhta...',\n",
       "  'snippet': 'MBZUAI (Mohamed bin Zayed University of Artificial Intelligence) ... In this paper, We propose a self supervised contrastive learning method to segment the ...',\n",
       "  'snippet_highlighted_words': ['Artificial Intelligence',\n",
       "   'self supervised',\n",
       "   'learning'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/randmuhtaseb',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac32ae2e97d4bd165d615c2a640073ec3fd2d61295c70d5b1aada69f5148e931223.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/randmuhtaseb&tbm=ilp&ilps=ADJL0izbJ-MDnyBl7y7DuLKkKiWWIJ-Ymg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0izbJ-MDnyBl7y7DuLKkKiWWIJ-Ymg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Frandmuhtaseb',\n",
       "  'cached_page_link': '/search?q=related:https://jo.linkedin.com/in/randmuhtaseb',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 6,\n",
       "  'title': 'Hasan Mohammad Al-Oqool - Artificial Intelligence Trainer',\n",
       "  'link': 'https://jo.linkedin.com/in/hasan-mohammad-al-oqool-8717b9191',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Machine Learning Engineer (Artificial Intelligence Trainer at Tahaluf) ... knowledge I have about these fields are all self-studied and University courses.',\n",
       "  'snippet_highlighted_words': ['Machine Learning',\n",
       "   'Artificial Intelligence',\n",
       "   'self'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Artificial Intelligence Trainer',\n",
       "     'Tahaluf Al Emarat Technical Solutions    ']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/hasan-mohammad-al-oqool-8717b9191',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac34204385c585fcf36f2f37040522405163c59df450e173675bab5ac981f671cce.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/hasan-mohammad-al-oqool-8717b9191&tbm=ilp&ilps=ADJL0iySjr9kCvbGaAN0NcIt5jxAu29XOg',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iySjr9kCvbGaAN0NcIt5jxAu29XOg&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fhasan-mohammad-al-oqool-8717b9191',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 7,\n",
       "  'title': 'Yara Al-Rahahleh - Developer Associate - PwC Middle East',\n",
       "  'link': 'https://jo.linkedin.com/in/yara-al-rahahleh',\n",
       "  'displayed_link': 'https://jo.linkedin.com  yara-al-rahahleh',\n",
       "  'snippet': 'I am a motivated fresh graduate computer engineer, interested in artificial intelligence and machine learning products. Tweaked deep-learning systems and ...',\n",
       "  'snippet_highlighted_words': ['artificial intelligence and machine learning',\n",
       "   'deep',\n",
       "   'learning'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Developer Associate',\n",
       "     'PwC Middle East']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/yara-al-rahahleh',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac3e80b827d5e639b7ace45f91d0333627cef9627df3038e0d1600293948c5ed607.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/yara-al-rahahleh&tbm=ilp&ilps=ADJL0iwgea88oZh1fBGb4fK4TcWsUyot3A',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iwgea88oZh1fBGb4fK4TcWsUyot3A&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fyara-al-rahahleh',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 8,\n",
       "  'title': 'Abdalrahman Shahrour - Artificial Intelligence Researcher',\n",
       "  'link': 'https://jo.linkedin.com/in/shahrour',\n",
       "  'displayed_link': 'https://jo.linkedin.com  shahrour',\n",
       "  'snippet': '  Deep Learning for Natural Language Processing    Transformers in Computer Vision    IBM AI Engineering Professional Certificate (V2) ...',\n",
       "  'snippet_highlighted_words': ['Deep Learning', 'Transformers', 'AI'],\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/shahrour',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac39d4cc82bd2b3a3c321ae26fcd7ed9c8bea444ca3fcf8abed3b2515b3d6f986b1.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/shahrour&tbm=ilp&ilps=ADJL0iyTFZIp3ILYrBhUFzDpoJ9WlSyO-w',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iyTFZIp3ILYrBhUFzDpoJ9WlSyO-w&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fshahrour',\n",
       "  'source': 'linkedin.com'},\n",
       " {'position': 9,\n",
       "  'title': 'Ayman Al-Bitar - Data Scientist - Orange Jordan',\n",
       "  'link': 'https://jo.linkedin.com/in/ayman-al-bitar-7208baa',\n",
       "  'displayed_link': 'https://jo.linkedin.com  ...',\n",
       "  'snippet': 'Data Scientist | MLOPS| Deep Learning | ERP techno-functional Consultant ...   Data Science: Supervised Machine Learning in Python ...',\n",
       "  'snippet_highlighted_words': ['Deep Learning',\n",
       "   'Supervised Machine Learning'],\n",
       "  'rich_snippet': {'top': {'extensions': [' ',\n",
       "     'Data Scientist',\n",
       "     'Orange Jordan']}},\n",
       "  'about_this_result': {'source': {'description': 'LinkedIn is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft.',\n",
       "    'source_info_link': 'https://jo.linkedin.com/in/ayman-al-bitar-7208baa',\n",
       "    'security': 'secure',\n",
       "    'icon': 'https://serpapi.com/searches/644f6f539c84ac0458dda1f6/images/76c5ee770734c44f509884251edf3ac36d072b261266b64235b6c5419e0358917933cb1d0f5b91cf56aa09e891d30329.png'}},\n",
       "  'about_page_link': 'https://www.google.com/search?q=About+https://jo.linkedin.com/in/ayman-al-bitar-7208baa&tbm=ilp&ilps=ADJL0iw7t0JUvrOYla8K-rFlNHIXN20HTA',\n",
       "  'about_page_serpapi_link': 'https://serpapi.com/search.json?engine=google_about_this_result&google_domain=google.com&ilps=ADJL0iw7t0JUvrOYla8K-rFlNHIXN20HTA&q=About+https%3A%2F%2Fjo.linkedin.com%2Fin%2Fayman-al-bitar-7208baa',\n",
       "  'source': 'linkedin.com'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get(\"organic_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "091355ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current': 1,\n",
       " 'next': 'https://www.google.com/search?q=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&oq=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&start=10&sourceid=chrome&ie=UTF-8',\n",
       " 'other_pages': {'2': 'https://www.google.com/search?q=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&oq=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&start=10&sourceid=chrome&ie=UTF-8',\n",
       "  '3': 'https://www.google.com/search?q=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&oq=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&start=20&sourceid=chrome&ie=UTF-8',\n",
       "  '4': 'https://www.google.com/search?q=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&oq=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&start=30&sourceid=chrome&ie=UTF-8',\n",
       "  '5': 'https://www.google.com/search?q=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&oq=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&start=40&sourceid=chrome&ie=UTF-8'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get(\"pagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad660fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2068822889.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[42], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://serpapi.com/1835fadd674bcb5f97ec8bbf2c66cd388599e307ab40ab27f3d49b0f8345e107/search.json?device=desktop&engine=google&google_domain=google.com&q=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&start=10\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "https://serpapi.com/1835fadd674bcb5f97ec8bbf2c66cd388599e307ab40ab27f3d49b0f8345e107/search.json?device=desktop&engine=google&google_domain=google.com&q=%0Asite%3Ajo.linkedin.com%2Fin+AI+AND+Machine+Learning+AND+%28Transformers+OR+Self+Superfised+learning%29%0A&start=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e2139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
